---
title: "Comparion Using Interpolation"
author: "Ziang Zhang"
date: "16/09/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aghq)
library(TMB)
library(Matrix)
n_samp = 10000
compute_H_rue <- function(d,n){
  H <- matrix(data = 0, nrow = n, ncol = n)
  for (i in 2:(nrow(H)-1)) {
    H[i,i] <- -(1/d[i-1]+1/d[i])
    H[i,i-1] <- 1/d[i-1]
    H[i,i+1] <- 1/d[i]
  }
  H
}
compute_B <- function(d,n){
  B <-matrix(0, nrow = n, ncol = n)
  B[1,1] <- d[1]/3
  B[1,2] <- d[1]/6
  B[n,n-1] <- d[n-1]/6
  B[n,n] <- d[n-1]/3
  for (i in 2:(nrow(B)-1)) {
    B[i,i-1] <- d[i-1]/6
    B[i,i] <- (d[i-1]+d[i])/3
    B[i,i+1] <- d[i]/6
  }
  B
}
compute_A <- function(d,n){
  A <-matrix(0, nrow = n, ncol = n)
  A[1,1] <- d[1]/2
  A[n,n] <- d[n-1]/2
  for (i in 2:(nrow(A)-1)) {
    A[i,i] <- (d[i-1]+d[i])/2
  }
  A
}


switch_matrix_once <- function(M, from, to){
  L <- diag(1, nrow = nrow(M), ncol = nrow(M))
  original_row <- L[to,]
  L[to,] <- L[from,]
  L[from,] <- original_row
  L%*%M%*%t(L)
}


switch_matrix <- function(z, x, M){
  all_grid <- sort(c(z,x))
  n <- length(x)
  for (i in 1:length(z)) {
    z_location <- which(all_grid %in% z[i])
    M <- switch_matrix_once(M,from = z_location, to = n + i)
    all_grid[z_location] <- all_grid[n+i]
    all_grid[n + i] <- z[i]
  }
  for (j in 1:length(x)) {
    x_location <- which(all_grid %in% x[j])
    M <- switch_matrix_once(M,from = x_location, to = j)
    all_grid[x_location] <- all_grid[j]
    all_grid[j] <- x[j]
  }
  M
}


Interpolation_vec_v1 <- function(t, x_grid, gx, GP, n_samples = 500){
  all_grid <- sort(c(t,x_grid))
  t_location <- which(all_grid %in% t)
  n <- length(all_grid)
  all_d <- diff(all_grid)
  H <- compute_H_rue(all_d, n)
  B <- compute_B(all_d, n)
  A <- compute_A(all_d, n)
  if(GP == "RW2"){
    Q <- t(H) %*% solve(A) %*% H
  }
  else{
    D <- H[-c(1,length(all_grid)),]
    R <- B[-c(1,length(all_grid)), -c(1,length(all_grid))]
    Q <- t(D) %*% solve(R) %*% D
  }
  
  C <- solve(Q + Diagonal(n, x = 0.00001))
  C_trans <- switch_matrix(t,x_grid,C)
  Q_trans <- forceSymmetric(solve(C_trans))
  QAA <- Q_trans[(length(x_grid) + 1):n, (length(x_grid) + 1):n]
  QAB <- Q_trans[(length(x_grid) + 1):n, 1:(length(x_grid))]
  QBB <- Q[1:(length(x_grid)), 1:(length(x_grid))]
  compute_conditional_mean <- function(gx){
    conditional_mean <- -solve(QAA)%*% QAB %*% (gx)
    conditional_mean
  }
  conditional_mean_list <- apply(gx, 2, compute_conditional_mean)

  simulated_gz <- function(cmu){
    Lt <- chol(QAA)
    z <- rnorm(n = length(t))
    v <- solve(a = Lt, b = z)
    v + cmu
  }
  
  samples <- lapply(conditional_mean_list, simulated_gz)
  samples
}

```

# Simulation Setting

In this simulation study, we aim to compare the performance of RW2 method with ARIMA method, considering that there are some covariate values with missing observations. 

We consider two different types of settings for simulations. In the first type of setting, we fix the region of interest to $[0,100]$, and vary the number of observations $n$ between $\{10, 50, 100\}$ in that fixed interval. In the second type of setting, we fix the number of observations to $n = 50$, but vary the length of the region of interest. For simplicity, we consider the spacing between locations to be equal in all the simulation study.




The simulated data set has the form of $\{(x_i,y_i):i\in[n]\}$, where $x_i$ denotes the i-th (observed) covariate value and $y_i$ denotes its corresponding observation. For each simulated data set, we assume that there exists a set of covariate values $\{z_i:i\in[n]\}$ being disjoint of the observed covariate values. To compare the two Bayesian smoothing methods, we consider both the inference at the observed values (i.e. $g(\boldsymbol{x}):=\{g(x_i):i\in[n]\}$, and its interpolation values $g(\boldsymbol{z}):=\{g(z_i):i\in[n]\}$.

To obtain samples of the interpolation values $g(\boldsymbol{z})$, we first draw samples $\tilde{g}(\boldsymbol{x})$ from the posterior of $g(\boldsymbol{x})$, then sample from the conditional distribution of $g(\boldsymbol{z})|\tilde{g}(\boldsymbol{x})$ given by the prior distribution.

For the true function $g(.)$, we consider it being the function $$g(x) = 5\sin(0.1x),$$ observed at $x \in [0,100]$. 

\newpage

# Sample Size being 10

## RW2 Method

```{r echo=FALSE, fig.show="hold", out.width="50%"}

### Simulation with very sparse data: (n = 20)
n <- 10
x <- seq(5,100, by = 10)
compute_g <- function(x){
  5*sin(0.1*x)
}
y <- compute_g(x) + rnorm(n, sd = 3)
d <- diff(x)
X <- as(as.matrix(Diagonal(n)), "dgTMatrix")


H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
A <- compute_A(d, n = length(x))


###### RW2:
Q1 <- t(H) %*% solve(A) %*% H
Q1 <- as(Q1 + Diagonal(n, x = 0.0001), "dgTMatrix")

dyn.load(dynlib("02_RW2Comparison"))


tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q1,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q1,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)

tmbparams <- list(
  W = rep(0, n), # W = c(U); U = B-Spline coefficients
  theta1 = 0, # -2log(sigma)
  theta2 = 0
)

ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(2,100, by = 10)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "RW2")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-12,12), lty = 1)
points(mean_x ~ x, col = "red")
lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "RW2 for very sparse covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-12,12), lty = 1)
points(mean_z ~ z_grid, col = "red")

lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "RW2 for very sparse covariate: Interpolation")


```


The inferred results using RW2 method are summarized at above. The red line represents the posterior mean of $g(\boldsymbol{x})$ (or $g(\boldsymbol{z})$), the orange lines represent its 95 \% credible interval, and the light blue lines are the sample paths simulated from the posterior distribution of $g(\boldsymbol{x})$ (or $g(\boldsymbol{z})$). The black dotted line is the true function.


For both inferences, we computed the mean squared error (MSE) using posterior mean and mean credible width (MCW) using their 95% intervals. For the observed locations, we have MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.



## ARIMA Method

```{r echo=FALSE, fig.show="hold", out.width="50%"}
D <- H[-c(1,n),]
R <- B[-c(1,n), -c(1,n)]

Q3 <- t(D) %*% solve(R) %*% D
Q3 <- as(as.matrix(Q3 + Diagonal(n, x = 0.0001)), "dgTMatrix")

tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q3,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q3,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)


ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(2,100, by = 10)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "ARIMA")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-12,12), lty = 1)
points(mean_x ~ x, col = 'red')
lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "ARIMA for very sparse covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-12,12), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "ARIMA for very sparse covariate: Interpolation")

```

For the ARIMA approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.

\newpage


# Sample Size being 50

## RW2 Method

```{r echo=FALSE, fig.show="hold", out.width="50%"}
### Simulation with sparse data: (n = 50)
n <- 50
n_samp <- 8000
x <- seq(1,99, by = 2)
compute_g <- function(x){
  5*sin(0.1*x)
}
y <- compute_g(x) + rnorm(n, sd = 3)
d <- diff(x)
X <- as(as.matrix(Diagonal(n)), "dgTMatrix")


H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
A <- compute_A(d, n = length(x))


###### RW2:
Q1 <- t(H) %*% solve(A) %*% H
Q1 <- as(Q1 + Diagonal(n, x = 0.0001), "dgTMatrix")



tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q1,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q1,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)

tmbparams <- list(
  W = rep(0, n), # W = c(U); U = B-Spline coefficients
  theta1 = 0, # -2log(sigma)
  theta2 = 0
)

ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(2,100, by = 2)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "RW2")
gz <- Matrix(0,nrow = 50, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = "red")
lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "RW2 for sparse covariate: Inference")
## 0.644
## 5.058



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = "red")

lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "RW2 for sparse covariate: Interpolation")
## 0.609
## 5.199
```


For the RW2 approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.


## ARIMA Method

```{r echo=FALSE, fig.show="hold", out.width="50%"}

###### ARIMA:
D <- H[-c(1,n),]
R <- B[-c(1,n), -c(1,n)]

Q3 <- t(D) %*% solve(R) %*% D
Q3 <- as(as.matrix(Q3 + Diagonal(n, x = 0.0001)), "dgTMatrix")

tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q3,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q3,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)


ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(2,100, by = 2)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "ARIMA")
gz <- Matrix(0,nrow = 50, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = "red")

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "ARIMA for sparse covariate: Inference")
## 0.638
## 5.033



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = "red")

lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "ARIMA for sparse covariate: Interpolation")
 ## 0.609
## 5.199

```

For the ARIMA approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.

\newpage


# Sample Size being 100


## RW2

```{r echo=FALSE, fig.show="hold", out.width="50%"}
n <- 100
x <- seq(0.5,99.5, by = 1)
compute_g <- function(x){
  5*sin(0.1*x)
}
y <- compute_g(x) + rnorm(n, sd = 3)
d <- diff(x)
X <- as(as.matrix(Diagonal(n)), "dgTMatrix")


H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
A <- compute_A(d, n = length(x))


###### RW2:
Q1 <- t(H) %*% solve(A) %*% H
Q1 <- as(Q1 + Diagonal(n, x = 0.0001), "dgTMatrix")


tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q1,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q1,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)

tmbparams <- list(
  W = rep(0, n), # W = c(U); U = B-Spline coefficients
  theta1 = 0, # -2log(sigma)
  theta2 = 0
)

ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(0,99, by = 1)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "RW2")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "RW2 for dense covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "RW2 for dense covariate: Interpolation")


```


For the RW2 approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.


## ARIMA

```{r echo=FALSE, fig.show="hold", out.width="50%"}

###### ARIMA:
D <- H[-c(1,n),]
R <- B[-c(1,n), -c(1,n)]

Q3 <- t(D) %*% solve(R) %*% D
Q3 <- as(as.matrix(Q3 + Diagonal(n, x = 0.0001)), "dgTMatrix")

tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q3,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q3,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)


ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(0,99, by = 1)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "ARIMA")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "ARIMA for dense covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "ARIMA for dense covariate: Interpolation")

```

For the ARIMA approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.

\newpage







# Study Region [0,200]


## RW2

```{r echo=FALSE, fig.show="hold", out.width="50%"}
n <- 50
x <- seq(2,200, length.out = 50)
compute_g <- function(x){
  5*sin(0.1*x)
}
y <- compute_g(x) + rnorm(n, sd = 3)
d <- diff(x)
X <- as(as.matrix(Diagonal(n)), "dgTMatrix")


H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
A <- compute_A(d, n = length(x))


###### RW2:
Q1 <- t(H) %*% solve(A) %*% H
Q1 <- as(Q1 + Diagonal(n, x = 0.0001), "dgTMatrix")


tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q1,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q1,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)

tmbparams <- list(
  W = rep(0, n), # W = c(U); U = B-Spline coefficients
  theta1 = 0, # -2log(sigma)
  theta2 = 0
)

ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(1,199, length.out = 50)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "RW2")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "RW2 for very sparse covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "RW2 for very sparse covariate: Interpolation")


```


For the RW2 approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.


## ARIMA

```{r echo=FALSE, fig.show="hold", out.width="50%"}

###### ARIMA:
D <- H[-c(1,n),]
R <- B[-c(1,n), -c(1,n)]

Q3 <- t(D) %*% solve(R) %*% D
Q3 <- as(as.matrix(Q3 + Diagonal(n, x = 0.0001)), "dgTMatrix")

tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q3,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q3,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)


ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(1,199, length.out = 50)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "ARIMA")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "ARIMA for very sparse covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "ARIMA for very sparse covariate: Interpolation")

```

For the ARIMA approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.



\newpage




# Study Region [0,100]


## RW2

```{r echo=FALSE, fig.show="hold", out.width="50%"}
n <- 50
x <- seq(0,100, length.out = 50)
compute_g <- function(x){
  5*sin(0.1*x)
}
y <- compute_g(x) + rnorm(n, sd = 3)
d <- diff(x)
X <- as(as.matrix(Diagonal(n)), "dgTMatrix")


H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
A <- compute_A(d, n = length(x))


###### RW2:
Q1 <- t(H) %*% solve(A) %*% H
Q1 <- as(Q1 + Diagonal(n, x = 0.0001), "dgTMatrix")


tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q1,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q1,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)

tmbparams <- list(
  W = rep(0, n), # W = c(U); U = B-Spline coefficients
  theta1 = 0, # -2log(sigma)
  theta2 = 0
)

ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(1,99, length.out = 50)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "RW2")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "RW2 for sparse covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "RW2 for sparse covariate: Interpolation")


```


For the RW2 approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.


## ARIMA

```{r echo=FALSE, fig.show="hold", out.width="50%"}

###### ARIMA:
D <- H[-c(1,n),]
R <- B[-c(1,n), -c(1,n)]

Q3 <- t(D) %*% solve(R) %*% D
Q3 <- as(as.matrix(Q3 + Diagonal(n, x = 0.0001)), "dgTMatrix")

tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q3,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q3,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)


ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(1,99, length.out = 50)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "ARIMA")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "ARIMA for sparse covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "ARIMA for sparse covariate: Interpolation")

```

For the ARIMA approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.



\newpage


# Study Region [30,80]


## RW2

```{r echo=FALSE, fig.show="hold", out.width="50%"}
n <- 50
x <- seq(31,80, length.out = 50)
compute_g <- function(x){
  5*sin(0.1*x)
}
y <- compute_g(x) + rnorm(n, sd = 3)
d <- diff(x)
X <- as(as.matrix(Diagonal(n)), "dgTMatrix")


H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
H <- compute_H_rue(d,n = length(x))
B <- compute_B(d,n = length(x))
A <- compute_A(d, n = length(x))


###### RW2:
Q1 <- t(H) %*% solve(A) %*% H
Q1 <- as(Q1 + Diagonal(n, x = 0.0001), "dgTMatrix")


tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q1,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q1,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)

tmbparams <- list(
  W = rep(0, n), # W = c(U); U = B-Spline coefficients
  theta1 = 0, # -2log(sigma)
  theta2 = 0
)

ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(31.5,80.5, length.out = 50)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "RW2")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "RW2 for dense covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "RW2 for dense covariate: Interpolation")


```


For the RW2 approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.


## ARIMA

```{r echo=FALSE, fig.show="hold", out.width="50%"}

###### ARIMA:
D <- H[-c(1,n),]
R <- B[-c(1,n), -c(1,n)]

Q3 <- t(D) %*% solve(R) %*% D
Q3 <- as(as.matrix(Q3 + Diagonal(n, x = 0.0001)), "dgTMatrix")

tmbdat <- list(
  # Design matrix
  X = X,
  # Penalty(Precision) matrix
  P = Q3,
  # Log determinant of penalty matrix (without the sigma part)
  logPdet = as.numeric(determinant(Q3,logarithm = TRUE)$modulus),
  # Response
  y = y,
  # PC Prior params
  u1 = 5,
  alpha1 = 0.1,
  u2 = 5,
  alpha2 = 0.1
)


ff <- TMB::MakeADFun(
  data = tmbdat,
  parameters = tmbparams,
  random = "W",
  DLL = "02_RW2Comparison",
  silent = TRUE
)

# Hessian not implemented for RE models
ff$he <- function(w) numDeriv::jacobian(ff$gr,w)
# AGHQ
set.seed(123)
quad <- aghq::marginal_laplace_tmb(ff,7,c(0,0))


### z_grid
z_grid <- seq(31.5,80.5, length.out = 50)
gx <- sample_marginal(quad, n_samp)
gx <- gx$samps
gz_list <- Interpolation_vec_v1(t = z_grid, x, gx, "ARIMA")
gz <- Matrix(0,nrow = n, ncol = n_samp)
for (i in 1:length(gz_list)) {
  gz[,i] <- gz_list[[i]]
}

mean_x <- apply(gx, 1, mean)
upper_x <- apply(gx, 1, quantile, p = 0.975)
lower_x <- apply(gx, 1, quantile, p = 0.025)

mean_z <- apply(gz, 1, mean)
upper_z <- apply(gz, 1, quantile, p = 0.975)
lower_z <- apply(gz, 1, quantile, p = 0.025)

### Observed grid:
plot(mean_x ~ x, type = 'l', xlab = "observed grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_x ~ x, col = 'red')

lines(upper_x ~ x, lty = 2, col = 'orange')
lines(lower_x ~ x, lty = 2, col = 'orange')
lines(compute_g(x) ~ x, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gx[,i] ~ x, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}
title(main = "ARIMA for dense covariate: Inference")



### Interpolated grid:
plot(mean_z ~ z_grid, type = 'l', xlab = "interpolated grid", ylab = "y", col = "red", ylim = c(-8,8), lty = 1)
points(mean_z ~ z_grid, col = 'red')
lines(upper_z ~ z_grid, lty = 2, col = 'orange')
lines(lower_z ~ z_grid, lty = 2, col = 'orange')
lines(compute_g(z_grid) ~ z_grid, lty = 3, col = 'black')
for (i in sample.int(n_samp,50)) {
  lines(gz[,i] ~ z_grid, col = rgb(0, 0, 255, max = 255, alpha = 20, names = "grey"))
}

title(main = "ARIMA for dense covariate: Interpolation")

```

For the ARIMA approach, inference of $g(\boldsymbol{x})$ has MSE being `r round(mean((mean_x - compute_g(x))^2),3)` and MCW being `r round(mean(upper_x - lower_x),3)`. For the interpolated locations, we have MSE being `r round(mean((mean_z - compute_g(z_grid))^2),3)` and MCW being `r round(mean(upper_z - lower_z),3)`.



# Conclusion:

From the first three set of simulation studies (where $n$ is changing from small to large), we can make the observation that, the two Bayesian smoothing methods perform similar in terms of MSE, but the ARIMA method gives smaller values of MCW. This difference gets larger as the sample size $n$ declines.

The same conclusion is also supported by the second sets of simulation studies (where the region of interest is changing). Unless the sample size is large enough and the spacing between locations is small, ARIMA method yields more favorable inferential result than the RW2 method.





